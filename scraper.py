import requests
from bs4 import BeautifulSoup
import os
import sys
import time
import shutil

# --- MOTOR: DrissionPage ---
from DrissionPage import ChromiumPage, ChromiumOptions

# --- BE√ÅLL√çT√ÅSOK ---
TOKEN = os.environ.get('TELEGRAM_TOKEN')
CHAT_ID = os.environ.get('TELEGRAM_CHAT_ID')
SEEN_FILE = "seen_ads.txt"
KEYWORDS_FILE = "keywords.txt"

# üåä AUTOMATA √ÅRV√çZV√âDELEM
# Ha egy kulcssz√≥ra egyszerre enn√©l t√∂bb tal√°lat van, azt "el≈ëzm√©ny bet√∂lt√©snek" vessz√ºk.
# Ilyenkor nem k√ºld egyes√©vel √©rtes√≠t√©st, csak egy √∂sszefoglal√≥t.
FLOOD_LIMIT = 3 

# URL-ek
URL_HA_SEARCH_BASE = "https://hardverapro.hu/aprok/keres.php?order=1&stext="
URL_MSZ = "https://www.menemszol.hu/aprohirdetes/page/1"

# --- K√ñZ√ñS SEG√âDF√úGGV√âNYEK ---

def send_telegram(message):
    if not TOKEN or not CHAT_ID:
        print("Hiba: Nincs be√°ll√≠tva TELEGRAM_TOKEN vagy TELEGRAM_CHAT_ID")
        return
    url = f"https://api.telegram.org/bot{TOKEN}/sendMessage"
    payload = {"chat_id": CHAT_ID, "text": message}
    try:
        requests.post(url, json=payload)
    except Exception as e:
        print(f"Hiba √ºzenetk√ºld√©skor: {e}")

def load_seen_ads():
    if not os.path.exists(SEEN_FILE):
        return set()
    with open(SEEN_FILE, "r") as f:
        return set(line.strip() for line in f)

def save_seen_ad(ad_url):
    with open(SEEN_FILE, "a") as f:
        f.write(ad_url + "\n")

def load_keywords_by_site():
    keywords = {"hardverapro": [], "menemszol": []}
    defaults = {"hardverapro": ["mac mini"], "menemszol": ["elektron", "access", "virus", "focusrite"]}

    if not os.path.exists(KEYWORDS_FILE):
        print("‚ö†Ô∏è Nem tal√°lhat√≥ a keywords.txt, alap√©rtelmezett szavakat haszn√°lom.")
        return defaults
    
    try:
        current_section = None
        with open(KEYWORDS_FILE, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line: continue
                if line.upper() == "[HARDVERAPRO]":
                    current_section = "hardverapro"
                    continue
                elif line.upper() == "[MENEMSZOL]":
                    current_section = "menemszol"
                    continue
                if current_section in keywords:
                    keywords[current_section].append(line.lower())
        
        if not keywords["hardverapro"]: keywords["hardverapro"] = defaults["hardverapro"]
        if not keywords["menemszol"]: keywords["menemszol"] = defaults["menemszol"]
        return keywords
    except Exception as e:
        print(f"Hiba a kulcsszavak olvas√°sakor: {e}")
        return defaults

# --- 1. HARDVERAPR√ì SCRAPER ---

def scrape_hardverapro(seen_ads, keywords):
    print("--- HardverApr√≥ ellen≈ërz√©se ---")
    
    ha_headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://hardverapro.hu/"
    }

    for keyword in keywords:
        search_term = f'"{keyword}"'
        print(f"üîé Keres√©s erre: {search_term}...")
        search_url = f"{URL_HA_SEARCH_BASE}{search_term}"
        
        try:
            response = requests.get(search_url, headers=ha_headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            ads = soup.find_all('li', class_='media')
            
            # --- GY≈∞JT√âS ---
            batch_new_items = []
            
            for ad in ads:
                title_div = ad.find('div', class_='uad-col-title')
                if not title_div: continue
                link_tag = title_div.find('a')
                if not link_tag: continue
                
                title = link_tag.get_text().strip()
                link = link_tag['href']
                full_link = link if link.startswith("http") else f"https://hardverapro.hu{link}"
                price_div = ad.find('div', class_='uad-price')
                price = price_div.get_text().strip() if price_div else "Nincs √°r"

                # C√≠m ellen≈ërz√©s (Szigor√∫ sz≈±r√©s)
                if keyword.lower() not in title.lower(): continue
                
                if full_link in seen_ads: continue 
                
                batch_new_items.append({
                    "title": title,
                    "price": price,
                    "link": full_link
                })

            # --- D√ñNT√âS ---
            count = len(batch_new_items)
            
            if count == 0:
                continue

            if count > FLOOD_LIMIT:
                print(f"üåä FLOOD DETEKT√ÅLVA ({count} db)! N√©ma ment√©s...")
                msg = f"‚ÑπÔ∏è **√öj kulcssz√≥ betan√≠tva:** '{keyword}'\n\nTal√°ltam {count} db r√©gi hirdet√©st a HardverApr√≥n. Ezeket elmentettem, de nem k√ºld√∂m el egyes√©vel."
                send_telegram(msg)
                for item in batch_new_items:
                    save_seen_ad(item['link'])
                    seen_ads.add(item['link'])
            else:
                for item in batch_new_items:
                    print(f"√öj HA tal√°lat: {item['title']}")
                    msg = f"üçé TAL√ÅLAT (HardverApr√≥ - {keyword})!\n\n**{item['title']}**\n√År: {item['price']}\n\nLink: {item['link']}"
                    send_telegram(msg)
                    save_seen_ad(item['link'])
                    seen_ads.add(item['link'])
            
            time.sleep(2)

        except Exception as e:
            print(f"HIBA a HardverApr√≥n√°l ({keyword}): {e}")

# --- 2. MENEMSZOL SCRAPER ---

def scrape_menemszol(seen_ads, keywords):
    print("--- Menemszol.hu ellen≈ërz√©se ---")
    page = None
    try:
        print("B√∂ng√©sz≈ë konfigur√°l√°sa...")
        co = ChromiumOptions()
        co.set_argument('--no-sandbox')
        co.set_argument('--headless=new')
        co.set_argument('--disable-gpu')
        co.set_user_agent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36')
        chrome_path = shutil.which("google-chrome") or shutil.which("chrome") or shutil.which("chromium")
        if chrome_path: co.set_paths(browser_path=chrome_path)

        page = ChromiumPage(co)
        print(f"Link megnyit√°sa: {URL_MSZ}")
        page.get(URL_MSZ)
        
        if "Verify" in page.title or "Just a moment" in page.title:
            try:
                time.sleep(2) 
                cf_box = page.ele('@id=challenge-stage', timeout=2); 
                if cf_box: cf_box.click() 
                verify_text = page.ele('text:Verify you are human', timeout=2)
                if verify_text: verify_text.click()
                time.sleep(5) 
            except: pass

        if "Just a moment" in page.title:
             print(f"‚ùå Cloudflare blokkol.")
             return

        print("‚úÖ Sikeresen bet√∂ltve!")
        soup = BeautifulSoup(page.html, 'html.parser')
        all_links = soup.find_all('a', href=True)
        
        # --- GY≈∞JT√âS ---
        batch_new_items = []

        for link in all_links:
            href = link['href']
            text = link.get_text(" ", strip=True)
            
            if "/aprohirdetes/" not in href: continue
            ignore_list = ["/category/", "/page/", "?sort", "&sort", "do=markRead", "/profile/"]
            if any(x in href for x in ignore_list): continue
            if not text or len(text) < 3: continue

            # Kulcssz√≥ keres√©s
            found_kw = None
            for kw in keywords:
                if kw in text.lower():
                    found_kw = kw
                    break
            
            if not found_kw: continue
            if href in seen_ads: continue
            if any(item['link'] == href for item in batch_new_items): continue

            batch_new_items.append({
                "title": text,
                "link": href,
                "keyword": found_kw
            })

        # --- D√ñNT√âS ---
        count = len(batch_new_items)
        print(f"  -> {count} db √∫j tal√°lat az oldalon.")

        if count > 5: # Menemszolon kicsit enged√©kenyebb a limit, mert egy oldalt n√©z√ºnk
             print(f"üåä FLOOD DETEKT√ÅLVA ({count} db)!")
             msg = f"‚ÑπÔ∏è **Menemszol import√°l√°s:**\n\nTal√°ltam {count} db olyan hirdet√©st, ami eddig nem volt meg. Ezeket n√©m√°n elmentettem."
             send_telegram(msg)
             for item in batch_new_items:
                 save_seen_ad(item['link'])
                 seen_ads.add(item['link'])
        else:
             for item in batch_new_items:
                print(f"√öj Menemszol tal√°lat: {item['title']}")
                msg = f"üéπ TAL√ÅLAT (Menemszol)!\n\n**{item['title']}**\n\nLink: {item['link']}"
                send_telegram(msg)
                save_seen_ad(item['link'])
                seen_ads.add(item['link'])

        print(f"Menemszol v√©ge.")

    except Exception as e:
        print(f"KRITIKUS HIBA a Menemszoln√°l: {e}")
    finally:
        if page:
            try: page.quit()
            except: pass

# --- F≈ê PROGRAM ---

if __name__ == "__main__":
    seen_ads_memory = load_seen_ads()
    all_keywords = load_keywords_by_site()
    
    scrape_hardverapro(seen_ads_memory, all_keywords['hardverapro'])
    print("-" * 30)
    scrape_menemszol(seen_ads_memory, all_keywords['menemszol'])
